{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:28:52.453902Z",
     "iopub.status.busy": "2025-11-09T16:28:52.453608Z",
     "iopub.status.idle": "2025-11-09T16:28:52.694616Z",
     "shell.execute_reply": "2025-11-09T16:28:52.693563Z",
     "shell.execute_reply.started": "2025-11-09T16:28:52.453882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "import cv2\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "base_dir = \".\\Dataset\"\n",
    "# this is the main directory where all the files are stored\n",
    "\n",
    "output_dir = \".\\Process Folder\"\n",
    "# this is the working directory of the data\n",
    "\n",
    "splits = {\"Training\": \"training_words\", \"Validation\": \"validation_words\", \"Testing\": \"testing_words\"}\n",
    "# These are set as the image in training set are in Data/Training/training_words\n",
    "\n",
    "Ext = (\".png\", \".jpg\",\".jpeg\", \".bmp\", \".tiff\", \".tiff\")\n",
    "# This will be used as we will resize the image to 320 x 320\n",
    "target_size = 320\n",
    "\n",
    "output_suffix = \"gray_320\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:28:52.696873Z",
     "iopub.status.busy": "2025-11-09T16:28:52.696502Z",
     "iopub.status.idle": "2025-11-09T16:28:52.703907Z",
     "shell.execute_reply": "2025-11-09T16:28:52.702981Z",
     "shell.execute_reply.started": "2025-11-09T16:28:52.696848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Training are below: \n",
      "training_labels.csv\n",
      "training_words\n",
      "\n",
      "Files in Validation are below: \n",
      "validation_labels.csv\n",
      "validation_words\n",
      "\n",
      "Files in Testing are below: \n",
      "testing_labels.csv\n",
      "testing_words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split = [\"Training\", \"Validation\" ,\"Testing\"]\n",
    "for s in split:\n",
    "    print(f\"Files in {s} are below: \")\n",
    "    for file in os.listdir(base_dir + \"/\"+ s):\n",
    "        print(file)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:28:52.705169Z",
     "iopub.status.busy": "2025-11-09T16:28:52.704857Z",
     "iopub.status.idle": "2025-11-09T16:28:52.920064Z",
     "shell.execute_reply": "2025-11-09T16:28:52.918980Z",
     "shell.execute_reply.started": "2025-11-09T16:28:52.705142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the image is :  (84, 207, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is how a singe image looks like \n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(base_dir + \"/Training/training_words/1.png\")\n",
    "print(\"Shape of the image is : \", image.shape)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "plt.title(\"My Image\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:28:52.921072Z",
     "iopub.status.busy": "2025-11-09T16:28:52.920841Z",
     "iopub.status.idle": "2025-11-09T16:28:52.930977Z",
     "shell.execute_reply": "2025-11-09T16:28:52.929917Z",
     "shell.execute_reply.started": "2025-11-09T16:28:52.921052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_dir (input_fol, output_fol, target_size=320):\n",
    "    os.makedirs(output_fol, exist_ok=True)\n",
    "\n",
    "    image_files = []\n",
    "    for ext in Ext:\n",
    "        image_files.extend(glob(os.path.join(input_fol, f\"*{ext}\")))\n",
    "\n",
    "    print(f\"\\nInput folder : {input_fol}\")\n",
    "    print(f\"Output folder : {output_fol}\")\n",
    "    print(f\"No.of Files found : {len(image_files)}\")\n",
    "\n",
    "    for i, file_no in enumerate(image_files, start=1):\n",
    "        img = cv2.imread(file_no)\n",
    "\n",
    "        # If file is unavailable\n",
    "        if img is None:\n",
    "            print(f\"Failed to read Image at: {file_no}\")\n",
    "            continue\n",
    "\n",
    "        # Converting the image into grey Scale\n",
    "        grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculating a median of all the pixel intensities\n",
    "        # This will be used as the background color when padding \n",
    "        # the image, making the background blend smoothly.\n",
    "        bg_color = int(np.median(grey))\n",
    "\n",
    "        # The original size of the image, It will be 2D as it is Grey scale\n",
    "        h, w = grey.shape\n",
    "        # Calculates a scaling factor to resize the image so that neither width nor height exceeds target_size.\n",
    "        scale = min(target_size/w, target_size/h)\n",
    "        # Computes the new dimensions by multiplying the original width and height by the scale.\n",
    "        # new_w, new_h = max(1, (w*scale)), max(1, (h*scale))\n",
    "        # resized = cv2.resize(grey, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        new_w, new_h = int(max(1, w * scale)), int(max(1, h * scale))\n",
    "        resized = cv2.resize(grey, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "\n",
    "        canvas = np.full((target_size, target_size), bg_color, dtype=np.uint8)       \n",
    "\n",
    "        # Places the resized image in the center of the square canvas.\n",
    "        # \tCalculates the horizontal offset needed to center the image.\n",
    "        x_offset = (target_size - new_w) // 2\n",
    "        # - Calculates the vertical offset needed to center the image.\n",
    "        y_offset = (target_size - new_h) // 2\n",
    "        # Places the resized image onto the canvas at the calculated position.\n",
    "        # The canvas is assumed to be a NumPy array of shape(target_size, target_size.\n",
    "        canvas[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
    "\n",
    "        # Extracts the base name (just the file name) from the original file path\n",
    "        filename = os.path.basename(file_no)\n",
    "        # Constructs the full path where the image will be saved.\n",
    "        save_path = os.path.join(output_fol, filename)\n",
    "        # - Uses OpenCV to write the image data in canvas to the file at save_path\n",
    "        cv2.imwrite(save_path, canvas)\n",
    "\n",
    "        if i % 100 == 0 or i == len(image_files):\n",
    "            print(f\"[{i}/{len(image_files)}] ‚úÖ Processed\")\n",
    "            # Prints progress after every 50 images or at the end of the process.\n",
    "\n",
    "    print(f\"üéâ Finished: {input_folder} ‚Üí {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:28:52.933012Z",
     "iopub.status.busy": "2025-11-09T16:28:52.932097Z",
     "iopub.status.idle": "2025-11-09T16:29:15.426631Z",
     "shell.execute_reply": "2025-11-09T16:29:15.425761Z",
     "shell.execute_reply.started": "2025-11-09T16:28:52.932984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input folder : .\\Dataset\\Training\\training_words\n",
      "Output folder : .\\Process Folder\\Training\\training_words_gray_320_320\n",
      "No.of Files found : 3120\n",
      "[100/3120] ‚úÖ Processed\n",
      "[200/3120] ‚úÖ Processed\n",
      "[300/3120] ‚úÖ Processed\n",
      "[400/3120] ‚úÖ Processed\n",
      "[500/3120] ‚úÖ Processed\n",
      "[600/3120] ‚úÖ Processed\n",
      "[700/3120] ‚úÖ Processed\n",
      "[800/3120] ‚úÖ Processed\n",
      "[900/3120] ‚úÖ Processed\n",
      "[1000/3120] ‚úÖ Processed\n",
      "[1100/3120] ‚úÖ Processed\n",
      "[1200/3120] ‚úÖ Processed\n",
      "[1300/3120] ‚úÖ Processed\n",
      "[1400/3120] ‚úÖ Processed\n",
      "[1500/3120] ‚úÖ Processed\n",
      "[1600/3120] ‚úÖ Processed\n",
      "[1700/3120] ‚úÖ Processed\n",
      "[1800/3120] ‚úÖ Processed\n",
      "[1900/3120] ‚úÖ Processed\n",
      "[2000/3120] ‚úÖ Processed\n",
      "[2100/3120] ‚úÖ Processed\n",
      "[2200/3120] ‚úÖ Processed\n",
      "[2300/3120] ‚úÖ Processed\n",
      "[2400/3120] ‚úÖ Processed\n",
      "[2500/3120] ‚úÖ Processed\n",
      "[2600/3120] ‚úÖ Processed\n",
      "[2700/3120] ‚úÖ Processed\n",
      "[2800/3120] ‚úÖ Processed\n",
      "[2900/3120] ‚úÖ Processed\n",
      "[3000/3120] ‚úÖ Processed\n",
      "[3100/3120] ‚úÖ Processed\n",
      "[3120/3120] ‚úÖ Processed\n",
      "üéâ Finished: .\\Dataset\\Training\\training_words ‚Üí .\\Process Folder\\Training\\training_words_gray_320_320\n",
      "\n",
      "Input folder : .\\Dataset\\Validation\\validation_words\n",
      "Output folder : .\\Process Folder\\Validation\\validation_words_gray_320_320\n",
      "No.of Files found : 780\n",
      "[100/780] ‚úÖ Processed\n",
      "[200/780] ‚úÖ Processed\n",
      "[300/780] ‚úÖ Processed\n",
      "[400/780] ‚úÖ Processed\n",
      "[500/780] ‚úÖ Processed\n",
      "[600/780] ‚úÖ Processed\n",
      "[700/780] ‚úÖ Processed\n",
      "[780/780] ‚úÖ Processed\n",
      "üéâ Finished: .\\Dataset\\Validation\\validation_words ‚Üí .\\Process Folder\\Validation\\validation_words_gray_320_320\n",
      "\n",
      "Input folder : .\\Dataset\\Testing\\testing_words\n",
      "Output folder : .\\Process Folder\\Testing\\testing_words_gray_320_320\n",
      "No.of Files found : 780\n",
      "[100/780] ‚úÖ Processed\n",
      "[200/780] ‚úÖ Processed\n",
      "[300/780] ‚úÖ Processed\n",
      "[400/780] ‚úÖ Processed\n",
      "[500/780] ‚úÖ Processed\n",
      "[600/780] ‚úÖ Processed\n",
      "[700/780] ‚úÖ Processed\n",
      "[780/780] ‚úÖ Processed\n",
      "üéâ Finished: .\\Dataset\\Testing\\testing_words ‚Üí .\\Process Folder\\Testing\\testing_words_gray_320_320\n"
     ]
    }
   ],
   "source": [
    "for s_name, images_subdir in splits.items():\n",
    "    # This converts mltiple paths into 1\n",
    "    # This gives images subdirectory\n",
    "    input_folder = os.path.join(base_dir, s_name, images_subdir)\n",
    "    \n",
    "    output_folder = os.path.join(output_dir, s_name, f\"{images_subdir}_{output_suffix}_{target_size}\")\n",
    "\n",
    "\n",
    "    if not os.path.isdir(input_folder):\n",
    "            print(f\"‚ö†Ô∏è Skip: folder not found ‚Üí {input_folder}\")\n",
    "            continue\n",
    "\n",
    "    process_dir(input_folder, output_folder, target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:15.428031Z",
     "iopub.status.busy": "2025-11-09T16:29:15.427686Z",
     "iopub.status.idle": "2025-11-09T16:29:19.630929Z",
     "shell.execute_reply": "2025-11-09T16:29:19.630142Z",
     "shell.execute_reply.started": "2025-11-09T16:29:15.427985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import torch as nn\n",
    "\n",
    "# This code is used to reproduce the results\n",
    "SEED = 50\n",
    "random.seed(SEED)\n",
    "# Sets the seed for NumPy‚Äôs random number generator.\n",
    "np.random.seed(SEED)\n",
    "# Sets the random seed for CPU-based operations in PyTorch ‚Äîthings like weight initialization or shuffling within a PyTorch dataset.\n",
    "torch.manual_seed(SEED)\n",
    "# Sets the same random seed for all available GPU devices (CUDA)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Checks if a GPU (cuda) is available; otherwise, it uses the CPU.\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# The image resolution (width √ó height) fed into the neural network.\n",
    "TARGET_SIZE   = (320, 320)  # <- change to 320x320\n",
    "BATCH_SIZE    = 32\n",
    "EPOCHS        = 10\n",
    "# Learning Rate\n",
    "LR            = 1e-3\n",
    "# Regularization term to prevent overfitting by penalizing large weights.\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "PATIENCE      = 5          # early stopping\n",
    "# Controls parallel data loading. 0 avoids multiprocessing errors\n",
    "NUM_WORKERS   = 0          # set 0 to be safe from DataLoader worker errors\n",
    "# Creates a folder named checkpoints_resnet18 to save model weights during training (so you can reload the best version later).\n",
    "SAVE_DIR      = \"./checkpoints_resnet18\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# These are mean and standard deviation values of the ImageNet dataset.\n",
    "# They‚Äôre used to normalize input images so that pretrained models (like ResNet) receive data in the same format as they were trained on originally.\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:19.632072Z",
     "iopub.status.busy": "2025-11-09T16:29:19.631733Z",
     "iopub.status.idle": "2025-11-09T16:29:19.639778Z",
     "shell.execute_reply": "2025-11-09T16:29:19.638911Z",
     "shell.execute_reply.started": "2025-11-09T16:29:19.632052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXISTS:  .\\Process Folder\\Training\\training_words_gray_320_320\n",
      "-----------------------------------------------------\n",
      "EXISTS:  .\\Process Folder\\Validation\\validation_words_gray_320_320\n",
      "-----------------------------------------------------\n",
      "EXISTS:  .\\Process Folder\\Testing\\testing_words_gray_320_320\n",
      "-----------------------------------------------------\n",
      "EXISTS:  .\\Dataset\\Training\\training_labels.csv\n",
      "-----------------------------------------------------\n",
      "EXISTS:  .\\Dataset\\Validation\\validation_labels.csv\n",
      "-----------------------------------------------------\n",
      "EXISTS:  .\\Dataset\\Testing\\testing_labels.csv\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This is where the preprocessed data is scored\n",
    "Data_dir = \".\\Process Folder\"\n",
    "\n",
    "# Path is provided for Training, Validation, and Testing\n",
    "# grey_320_320 indicate that images are converted to grey scale and into 320 x 320\n",
    "TRAIN_IMG_DIR = os.path.join(Data_dir, \"Training\",   \"training_words_gray_320_320\")\n",
    "VAL_IMG_DIR   = os.path.join(Data_dir, \"Validation\", \"validation_words_gray_320_320\")\n",
    "TEST_IMG_DIR  = os.path.join(Data_dir, \"Testing\",    \"testing_words_gray_320_320\")\n",
    "\n",
    "# Giving CSV file paths\n",
    "csv_base = \".\\Dataset\"\n",
    "\n",
    "TRAIN_CSV = os.path.join(csv_base, \"Training\",   \"training_labels.csv\")\n",
    "VAL_CSV   = os.path.join(csv_base, \"Validation\", \"validation_labels.csv\")\n",
    "TEST_CSV  = os.path.join(csv_base, \"Testing\",    \"testing_labels.csv\")\n",
    "\n",
    "for p in [TRAIN_IMG_DIR, VAL_IMG_DIR, TEST_IMG_DIR, TRAIN_CSV, VAL_CSV, TEST_CSV]:\n",
    "    print(\"EXISTS: \",p)\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:19.643311Z",
     "iopub.status.busy": "2025-11-09T16:29:19.643072Z",
     "iopub.status.idle": "2025-11-09T16:29:23.752225Z",
     "shell.execute_reply": "2025-11-09T16:29:23.751379Z",
     "shell.execute_reply.started": "2025-11-09T16:29:19.643292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "# When training deep learning models (like CNNs, RNNs, etc.), data is usually too large to load all at once. Dataset helps you:\n",
    "# Load data on demand (only when needed).\n",
    "# Apply transformations or preprocessing automatically.\n",
    "# Integrate easily with DataLoader to handle batching, shuffling, and parallel loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:23.753383Z",
     "iopub.status.busy": "2025-11-09T16:29:23.753052Z",
     "iopub.status.idle": "2025-11-09T16:29:23.762764Z",
     "shell.execute_reply": "2025-11-09T16:29:23.761604Z",
     "shell.execute_reply.started": "2025-11-09T16:29:23.753364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RXWordsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_path, img_dir, transform=None, label_encoder=None, fit_encoder=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # This checks if \"IMAGE\" col is present or not, else return a message\n",
    "        # assert condition, (if not met) result\n",
    "        assert \"IMAGE\" in self.df.columns, \"CSV must have an IMAGE column\"\n",
    "        assert \"MEDICINE_NAME\" in self.df.columns, \"CSV must have a MEDICINE NAME column\"\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        if label_encoder is None:\n",
    "            # label encoder is imported bcz we only have to import when not already imported\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            self.le = LabelEncoder()\n",
    "            fit_encoder = True\n",
    "        else:\n",
    "            self.le = label_encoder\n",
    "\n",
    "        # Fits the label encoder to the unique medicine names in the dataset.\n",
    "        if fit_encoder:\n",
    "            self.le.fit(self.df[\"MEDICINE_NAME\"].astype(str).values)\n",
    "\n",
    "        # Store Encoded Targets and Filenames\n",
    "        self.targets = self.le.transform(self.df[\"MEDICINE_NAME\"].astype(str).values)\n",
    "        self.images  = self.df[\"IMAGE\"].astype(str).values\n",
    "        self.classes = list(self.le.classes_)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Tells PyTorch how many samples exist in the dataset (so it can batch properly).\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Selects one image and its label by index.\n",
    "        img_name = self.images[idx]\n",
    "        y = self.targets[idx]\n",
    "        # Concatinate Image directory with name\n",
    "        path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Opens the image safely using Pillow.\n",
    "        if not os.path.exists(path):\n",
    "            # An error is raised when image in the Path doesnot exist\n",
    "            raise FileNotFoundError(f\"No image found: {path}\")\n",
    "        with Image.open(path) as im:\n",
    "            img = im.convert(\"RGB\")\n",
    "            # As resnet expects 3 channel input so data is converted into 3 channels \n",
    "\n",
    "        # Checks if a transformation function (e.g., resizing, normalization, flipping, etc.) has been provided when the dataset was created.\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, y\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline below does: \n",
    "- Resizes the image to (320, 320)\n",
    "\n",
    "- Randomly distorts color, position, and perspective to simulate real-world handwriting variation.\n",
    "\n",
    "- Converts to tensor and normalizes using ImageNet mean/std values (important for pretrained models like ResNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:23.764228Z",
     "iopub.status.busy": "2025-11-09T16:29:23.763776Z",
     "iopub.status.idle": "2025-11-09T16:29:24.354287Z",
     "shell.execute_reply": "2025-11-09T16:29:24.353333Z",
     "shell.execute_reply.started": "2025-11-09T16:29:23.764197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchvision import models\n",
    "\n",
    "# In PyTorch, transform.Compose is a utility provided by the 'torchvision.transforms' module that allows you to chain multiple image \n",
    "# transformations together into a single callable object. This is especially useful when preprocessing images for training deep learning models.\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # Resizes the image to a fixed size as RESNET requires fixed shape inputs. \n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    # 'RandomApply' = Randomly applies the listed transform 80% of the time. It adds variety without over-distortion\n",
    "    # 'ColorJitter' = Randomly changes brightness, contrast, saturation, hue\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02)], p=0.8),\n",
    "    # Makes the model rotation-invariant by rotating an image +-3 degrees.\n",
    "    transforms.RandomRotation(degrees=3),\n",
    "\n",
    "    # Applies small translations, shear, or scale changes, as it helps model to learn shape flexibility. \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.07, 0.07), shear=3, scale=(0.95, 1.05)),\n",
    "    # Randomly warps the image perspective which adds viewpoint variation\n",
    "    transforms.RandomPerspective(distortion_scale=0.15, p=0.5),\n",
    "    # Blurs the image slightly\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n",
    "    # Converts image ‚Üí PyTorch tensor (C √ó H √ó W) which is requried for model input\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizes pixels using ImageNet stats\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD), ])\n",
    "\n",
    "valtest_transform = transforms.Compose([\n",
    "    # Match model input size\n",
    "    transforms.Resize(TARGET_SIZE),\n",
    "    # Convert image to PyTorch tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Standardize pixel intensity distribution\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:24.355788Z",
     "iopub.status.busy": "2025-11-09T16:29:24.355200Z",
     "iopub.status.idle": "2025-11-09T16:29:24.413453Z",
     "shell.execute_reply": "2025-11-09T16:29:24.412461Z",
     "shell.execute_reply.started": "2025-11-09T16:29:24.355763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 78\n"
     ]
    }
   ],
   "source": [
    "# Fit LabelEncoder using TRAIN for consistency\n",
    "# reads your training CSV file that contains medicine names and image paths.\n",
    "df_train_tmp = pd.read_csv(TRAIN_CSV)\n",
    "# Convert Medicine name into IDs only for trainig data \n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_tmp[\"MEDICINE_NAME\"].astype(str).values)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Num classes:\", num_classes)\n",
    "\n",
    "train_ds = RXWordsDataset(TRAIN_CSV, TRAIN_IMG_DIR, transform=train_transform, label_encoder=le, fit_encoder=False)\n",
    "val_ds   = RXWordsDataset(VAL_CSV,   VAL_IMG_DIR,   transform=valtest_transform, label_encoder=le, fit_encoder=False)\n",
    "test_ds  = RXWordsDataset(TEST_CSV,  TEST_IMG_DIR,  transform=valtest_transform, label_encoder=le, fit_encoder=False)\n",
    "\n",
    "\n",
    "# How many images are fed to the model per iteration = Batch_size. This is done because dataset is large and we will import it per iteration\n",
    "# num_workerss are Number of CPU threads to load images in parallel\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:24.414610Z",
     "iopub.status.busy": "2025-11-09T16:29:24.414321Z",
     "iopub.status.idle": "2025-11-09T16:29:25.036748Z",
     "shell.execute_reply": "2025-11-09T16:29:25.035802Z",
     "shell.execute_reply.started": "2025-11-09T16:29:24.414590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:25.038058Z",
     "iopub.status.busy": "2025-11-09T16:29:25.037812Z",
     "iopub.status.idle": "2025-11-09T16:29:25.963697Z",
     "shell.execute_reply": "2025-11-09T16:29:25.962813Z",
     "shell.execute_reply.started": "2025-11-09T16:29:25.038034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=512, out_features=78, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# These weights were previously trained on ImageNet, so they already contain good feature extraction capability. \n",
    "# The path contains pretrained ResNet18 weights file\n",
    "MODEL_PATH = \"resnet18-f37072fd.pth\"\n",
    "# initialize the structure only, without loading ImageNet weights.\n",
    "model = models.resnet18(weights=None)\n",
    "\n",
    "# torch.load reads the weight file from disk and loads it into memory. It loads on CPU if GPU isn't available\n",
    "state_dict = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "# It then applies those learned weights to the ResNet18 model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# final layer in ResNet18 that performs classification. By default, it outputs 1000 classes but we replace it with no. of labels.\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Transfers the model to either GPU or CPU based on DEVICE.\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Loss, optimizer, dan scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# lr ‚Üí learning rate (how big each update step is)\n",
    "# weight_decay ‚Üí adds regularization to reduce overfitting.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# Automatically reduces the learning rate when validation accuracy stops improving.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2)\n",
    "# This displays the structure of the last fully connected layer after replacement.\n",
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:25.965185Z",
     "iopub.status.busy": "2025-11-09T16:29:25.964799Z",
     "iopub.status.idle": "2025-11-09T16:29:25.971798Z",
     "shell.execute_reply": "2025-11-09T16:29:25.970882Z",
     "shell.execute_reply.started": "2025-11-09T16:29:25.965161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    # Sets the model to training mode ‚Äî enables things like dropout and batch normalization updates.\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    # Loops through all batches from the DataLoader. It will give batch of images and a batch of image csv\n",
    "    for imgs, labels in loader:\n",
    "        # Moves the batch data to GPU (or CPU) for faster computation.\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # Clears any previous gradients stored in the optimizer (important to avoid accumulation).\n",
    "        optimizer.zero_grad()\n",
    "        # Feeds the images through the model to get raw predictions (logits) ‚Äî not probabilities yet.\n",
    "        logits = model(imgs)\n",
    "        # Calculates how far off the predictions are from the true labels using the loss function.\n",
    "        loss = criterion(logits, labels)\n",
    "        # Computes gradients for all model parameters (backpropagation).\n",
    "        loss.backward()\n",
    "        # Updates model weights using the computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Takes the index of the largest logit value ‚Üí predicted class label for each image.\n",
    "        preds = logits.argmax(1)\n",
    "        # Adds the batch loss to total loss (multiplied by batch size since .item() is per batch).\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += imgs.size(0)\n",
    "\n",
    "    return total_loss/total_samples, total_correct/total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:25.973206Z",
     "iopub.status.busy": "2025-11-09T16:29:25.972848Z",
     "iopub.status.idle": "2025-11-09T16:29:25.993478Z",
     "shell.execute_reply": "2025-11-09T16:29:25.992509Z",
     "shell.execute_reply.started": "2025-11-09T16:29:25.973177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This decorator disables gradient calculation\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        preds = logits.argmax(1)\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += imgs.size(0)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss/total_samples\n",
    "    avg_acc  = total_correct/total_samples\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return avg_loss, avg_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T16:29:25.994847Z",
     "iopub.status.busy": "2025-11-09T16:29:25.994504Z",
     "iopub.status.idle": "2025-11-09T16:38:41.182232Z",
     "shell.execute_reply": "2025-11-09T16:38:41.180760Z",
     "shell.execute_reply.started": "2025-11-09T16:29:25.994822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 13\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     va_loss, va_acc, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m     15\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(va_acc)\n",
      "Cell \u001b[1;32mIn[27], line 14\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Feeds the images through the model to get raw predictions (logits) ‚Äî not probabilities yet.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Calculates how far off the predictions are from the true labels using the loss function.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torchvision\\models\\resnet.py:270\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    268\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m    269\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m--> 270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torch\\nn\\modules\\activation.py:144\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Interns\\Github\\90-days-with-Buildables\\Doctor Project\\Hand written Detection\\lib\\site-packages\\torch\\nn\\functional.py:1695\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1695\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1697\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "best_path = os.path.join(SAVE_DIR, \"best_resnet18.pt\")\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [], \"train_acc\": [],\n",
    "    \"val_loss\": [], \"val_acc\": []\n",
    "}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    va_loss, va_acc, _, _ = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step(va_acc)\n",
    "\n",
    "    # simpan history\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"[Epoch {epoch:02d}/{EPOCHS}] \"\n",
    "          f\"train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | \"\n",
    "          f\"val_loss={va_loss:.4f} val_acc={va_acc:.4f} | \"\n",
    "          f\"time={(time.time()-t0):.1f}s\")\n",
    "    \n",
    "    # checkpoint\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"val_acc\": va_acc,\n",
    "            \"label_map\": list(le.classes_)\n",
    "        }, best_path)\n",
    "        print(f\"  -> Improved. Saved to {best_path}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping (no improve {PATIENCE} epochs).\")\n",
    "            break\n",
    "\n",
    "print(\"Best Val Acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-09T16:38:41.182938Z",
     "iopub.status.idle": "2025-11-09T16:38:41.183217Z",
     "shell.execute_reply": "2025-11-09T16:38:41.183104Z",
     "shell.execute_reply.started": "2025-11-09T16:38:41.183092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load best\n",
    "if os.path.exists(best_path):\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    print(f\"Loaded best from epoch {ckpt['epoch']} with val_acc={ckpt['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(\"WARNING: best checkpoint not found, using current weights.\")\n",
    "\n",
    "# Evaluate on TEST\n",
    "test_loss, test_acc, y_pred, y_true = evaluate(model, test_loader, criterion)\n",
    "print(\"\\n=== TEST METRICS ===\")\n",
    "print(f\"Test Loss : {test_loss:.4f}\")\n",
    "print(f\"Test Acc  : {test_acc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "target_names = list(le.classes_)\n",
    "report = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "cm_csv = os.path.join(SAVE_DIR, \"confusion_matrix_resnet18.csv\")\n",
    "cm_df.to_csv(cm_csv, encoding=\"utf-8-sig\")\n",
    "print(\"Confusion matrix saved to:\", cm_csv)\n",
    "\n",
    "# Simpan label map\n",
    "label_map_path = os.path.join(SAVE_DIR, \"label_map.json\")\n",
    "with open(label_map_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({i: cls for i, cls in enumerate(target_names)}, f, ensure_ascii=False, indent=2)\n",
    "print(\"Label map saved to:\", label_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-09T16:38:41.184044Z",
     "iopub.status.idle": "2025-11-09T16:38:41.184326Z",
     "shell.execute_reply": "2025-11-09T16:38:41.184195Z",
     "shell.execute_reply.started": "2025-11-09T16:38:41.184181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualisasi prediksi + generic dari mapping \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def _undo_norm_to_rgb(img_tensor):\n",
    "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    if img_np.shape[2] == 3:\n",
    "        img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
    "    else:\n",
    "        img_np = img_np * np.array(GRAY_STD) + np.array(GRAY_MEAN)\n",
    "        img_np = np.repeat(img_np, 3, axis=2)\n",
    "    return np.clip(img_np, 0, 1)\n",
    "\n",
    "def show_random_predictions_with_generic_map_single(\n",
    "    model, dataset, le, med2gen_map, num_images=12\n",
    "):\n",
    "    model.eval()\n",
    "    idxs = random.sample(range(len(dataset)), num_images)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img, y_med = dataset[idx]  # dataset single-label\n",
    "        with torch.no_grad():\n",
    "            out = model(img.unsqueeze(0).to(DEVICE))\n",
    "            pred_idx = out.argmax(1).item()\n",
    "\n",
    "        # convert ke gambar\n",
    "        img_np = _undo_norm_to_rgb(img)\n",
    "\n",
    "        true_med_str = le.classes_[y_med]\n",
    "        pred_med_str = le.classes_[pred_idx]\n",
    "\n",
    "        # GENERIC dari mapping\n",
    "        true_gen_str = med2gen_map.get(true_med_str, \"?\")\n",
    "        pred_gen_str = med2gen_map.get(pred_med_str, \"<NA>\")\n",
    "\n",
    "        # Warna hijau kalau MED benar\n",
    "        color = \"green\" if true_med_str == pred_med_str else \"red\"\n",
    "\n",
    "        title_lines = [\n",
    "            f\"MED  T: {true_med_str}\",\n",
    "            f\"MED  P: {pred_med_str}\",\n",
    "            f\"GEN  T: {true_gen_str}\",\n",
    "            f\"GEN  P: {pred_gen_str}\",\n",
    "        ]\n",
    "\n",
    "        plt.subplot(int(np.ceil(num_images/4)), 4, i+1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(\"\\n\".join(title_lines), color=color, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-09T16:38:41.186463Z",
     "iopub.status.idle": "2025-11-09T16:38:41.186873Z",
     "shell.execute_reply": "2025-11-09T16:38:41.186730Z",
     "shell.execute_reply.started": "2025-11-09T16:38:41.186677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def show_random_predictions_medicine_only(model, dataset, le, num_images=12):\n",
    "    model.eval()\n",
    "    idxs = random.sample(range(len(dataset)), num_images)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img, y_med = dataset[idx]\n",
    "        with torch.no_grad():\n",
    "            out = model(img.unsqueeze(0).to(DEVICE))\n",
    "            pred_idx = out.argmax(1).item()\n",
    "\n",
    "        # undo normalisasi\n",
    "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "        if img_np.shape[2] == 3:\n",
    "            img_np = img_np * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)\n",
    "        else:\n",
    "            img_np = img_np * np.array(GRAY_STD) + np.array(GRAY_MEAN)\n",
    "            img_np = np.repeat(img_np, 3, axis=2)\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "        true_med_str = le.classes_[y_med]\n",
    "        pred_med_str = le.classes_[pred_idx]\n",
    "\n",
    "        color = \"green\" if true_med_str == pred_med_str else \"red\"\n",
    "\n",
    "        plt.subplot(int(np.ceil(num_images/4)), 4, i+1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(f\"T: {true_med_str}\\nP: {pred_med_str}\",\n",
    "                  color=color, fontsize=9)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Cell C\n",
    "# show_random_predictions_with_generic_map_single(\n",
    "#     model, test_ds, le, med2gen_map, num_images=12\n",
    "# )\n",
    "# show_random_predictions_medicine_only(\n",
    "#     model, test_ds, le, num_images=12\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-09T16:38:41.187588Z",
     "iopub.status.idle": "2025-11-09T16:38:41.187896Z",
     "shell.execute_reply": "2025-11-09T16:38:41.187757Z",
     "shell.execute_reply.started": "2025-11-09T16:38:41.187746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-09T16:38:41.188858Z",
     "iopub.status.idle": "2025-11-09T16:38:41.189172Z",
     "shell.execute_reply": "2025-11-09T16:38:41.189009Z",
     "shell.execute_reply.started": "2025-11-09T16:38:41.188997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MedicinePredictor:\n",
    "    def __init__(self, checkpoint_path, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the predictor with a trained model checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to the best_resnet18.pt file\n",
    "            device: 'cuda' or 'cpu'\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        # Load checkpoint\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Get label mapping\n",
    "        self.label_map = checkpoint['label_map']\n",
    "        self.num_classes = len(self.label_map)\n",
    "        print(f\"Loaded {self.num_classes} medicine classes\")\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, self.num_classes)\n",
    "        \n",
    "        # Load weights\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully (trained on epoch {checkpoint['epoch']} with val_acc={checkpoint['val_acc']:.4f})\")\n",
    "        \n",
    "        # Define transforms (same as validation/test)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((320, 320)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def predict(self, image_path, top_k=5):\n",
    "        \"\"\"\n",
    "        Predict medicine name from an image.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            top_k: Return top K predictions with probabilities\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with prediction results\n",
    "        \"\"\"\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(img_tensor)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            # Get top K predictions\n",
    "            top_probs, top_indices = torch.topk(probs, k=min(top_k, self.num_classes), dim=1)\n",
    "            top_probs = top_probs.cpu().numpy()[0]\n",
    "            top_indices = top_indices.cpu().numpy()[0]\n",
    "        \n",
    "        # Format results\n",
    "        predictions = []\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            predictions.append({\n",
    "                'medicine_name': self.label_map[idx],\n",
    "                'confidence': float(prob),\n",
    "                'confidence_percent': f\"{prob*100:.2f}%\"\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'top_prediction': predictions[0]['medicine_name'],\n",
    "            'confidence': predictions[0]['confidence'],\n",
    "            'all_predictions': predictions\n",
    "        }\n",
    "    \n",
    "    def predict_batch(self, image_paths, top_k=5):\n",
    "        \"\"\"\n",
    "        Predict multiple images at once.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of image paths\n",
    "            top_k: Return top K predictions for each image\n",
    "            \n",
    "        Returns:\n",
    "            List of prediction results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                result = self.predict(img_path, top_k)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'image_path': img_path,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize predictor\n",
    "    checkpoint_path = \"./checkpoints_resnet18/best_resnet18.pt\"\n",
    "    predictor = MedicinePredictor(checkpoint_path, device='cpu')\n",
    "    \n",
    "    # Single prediction\n",
    "    image_path = \"path/to/your/image.png\"\n",
    "    \n",
    "    try:\n",
    "        result = predictor.predict(image_path, top_k=5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PREDICTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Image: {result['image_path']}\")\n",
    "        print(f\"\\nTop Prediction: {result['top_prediction']}\")\n",
    "        print(f\"Confidence: {result['confidence']*100:.2f}%\")\n",
    "        print(\"\\nTop 5 Predictions:\")\n",
    "        print(\"-\"*50)\n",
    "        for i, pred in enumerate(result['all_predictions'], 1):\n",
    "            print(f\"{i}. {pred['medicine_name']:20s} - {pred['confidence_percent']}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nPlease update 'image_path' with a valid image file path\")\n",
    "    \n",
    "    # Batch prediction example\n",
    "    # image_paths = [\"image1.png\", \"image2.png\", \"image3.png\"]\n",
    "    # batch_results = predictor.predict_batch(image_paths)\n",
    "    # for result in batch_results:\n",
    "    #     if 'error' in result:\n",
    "    #         print(f\"Error for {result['image_path']}: {result['error']}\")\n",
    "    #     else:\n",
    "    #         print(f\"{result['image_path']}: {result['top_prediction']} ({result['confidence']*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4982232,
     "sourceId": 8378585,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 240489910,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 104799,
     "modelInstanceId": 80386,
     "sourceId": 95833,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Hand written Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
